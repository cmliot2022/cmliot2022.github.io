<!DOCTYPE html>
<html lang="en">
  <head>
    <title>CML-IOT 2022</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link href="https://fonts.googleapis.com/css?family=Amatic+SC:400,700|Work+Sans:300,400,700" rel="stylesheet">
    <link rel="stylesheet" href="fonts/icomoon/style.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/magnific-popup.css">
    <link rel="stylesheet" href="css/jquery-ui.css">
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/bootstrap-datepicker.css">
    <link rel="stylesheet" href="css/animate.css">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mediaelement@4.2.7/build/mediaelementplayer.min.css">
    
    <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
  
    <link rel="stylesheet" href="css/aos.css">

    <link rel="stylesheet" href="css/style.css">
    
  </head>
  <body>
  
  <div class="site-wrap">

    <div class="site-mobile-menu">
      <div class="site-mobile-menu-header">
        <div class="site-mobile-menu-close mt-3">
          <span class="icon-close2 js-menu-toggle"></span>
        </div>
      </div>
      <div class="site-mobile-menu-body"></div>
    </div> <!-- .site-mobile-menu -->
    
    
    <div class="site-navbar-wrap js-site-navbar bg-white">
      
      <div class="container">
        <div class="site-navbar bg-light">
          <div class="py-1">
            <div class="row align-items-center">
              <div class="col-8">
                <h2 class="mb-0 site-logo"><a href="index.html">CML-IOT 2022</a></h2>
              </div>
              
            </div>
          </div>
        </div>
      </div>
    </div>
  
    
    
      
    <div class="site-blocks-cover overlay" style="background-image: url(images/Charles_River.jpg);" data-aos="fade" data-stellar-background-ratio="0.5">
      <div class="container">
        <div class="row align-items-center justify-content-center">
          <div class="col-md-10 text-center" data-aos="fade">
            <font size="10" color="white">The Fourth Workshop on Continual and Multimodal Learning for Internet of Things</font>
            <p class="mb-5">November 06, 2022 &bullet; Boston, Massachusetts, USA</p>
      <p class="mb-6">Co-Located with <a href="https://sensys.acm.org/2022/" style="color:white">SenSys 2022</a></p>
          </div>
        </div>
      </div>
    </div>  

<a name="more"></a>

      

    <div class="site-section">
      <div class="container">
		  
        
        <div class="row">
          <div class="col-md-12 mx-auto text-left section-heading">
		     <h3 class="mb-5 text-uppercase">Previous Editions: <a href="https://cmliot2021.github.io">CML-IOT'21</a>, <a href="https://cmliot2020.github.io">CML-IOT'20</a>, <a href="https://cmliot2019.github.io/index.html">CML-IOT'19</a></h3>  
		  </div>
		</div>
		
          
		  
		<div class="row">
		  <div class="col-md-12 mx-auto text-left section-heading">
             <h3 class="mb-5 text-uppercase">About CML-IOT </h3> 
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
		  
		  
  <p>The growth of the Internet of Things (IoT) has brought an ever-growing number of connected sensors, continuously streaming large quantities of multimodal data. These data come from a wide range of different sensing modalities and have distinct statistical characteristics over time, which are hardly captured by traditional learning methods. Continual and multimodal learning allows the integration, adaptation, and generalization of knowledge learned from experiential and heterogeneous data to new situations. Therefore, continual and multimodal learning is an important step to enable efficient information inference for IoT systems. CML-IOT welcomes works from diverse communities that introduce algorithmic and systemic approaches to leverage continual learning on multimodal data for applications and real-world computing systems for the Internet of Things.</p>
  <br />
    <br />
          </div>




          <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Call for Papers </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            <p>The Workshop on Continual and Multimodal Learning for Internet of Things (CML-IOT 2022) aims to explore the intersection of continual machine learning and multimodal modeling. The Internet of Things (IoT) has brought an ever-growing amount of multimodal sensing data (e.g., natural language, speech, image, video, audio, virtual reality, WiFi, GPS, RFID, vibration). The statistical properties of this data vary significantly over time and depending on the sensing modality; these differences are hardly captured by conventional learning methods. Continual and multimodal learning allows the integration, adaptation and generalization of knowledge learned from experiential and heterogenous data to new situations. Therefore, continual and multimodal learning is an important step to improve the estimation, utilization, and security of real-world data from IoT systems.</p>
			
			<p>We welcome works addressing these issues in different applications and domains, such as natural language processing, computer vision, human-centric sensing, smart cities, health, etc. We aim to bring together researchers from different areas to establish a multidisciplinary community and share the latest research. We focus on novel learning methods that can be applied on streaming multimodal data with applications to the Internet of Things. Topics of interest include, but are not limited to:</p>
  <li>Continual learning </li>
  <li>Transfer learning </li>
  <li>Federated learning </li>
  <li>Few-shot learning </li>
  <li>Multi-task learning </li>
  <li>Reinforcement learning </li>
  <li>Learning without forgetting </li>
  <li>Individual and/or institutional privacy </li>
  <li>Methods and architectures for partitioning on-device and off-device learning </li>
  <li>Managing high volumes of data flow</li>

        
   <br />
   
  <p>We also welcome continual learning methods that target: data distribution changes caused by the fast-changing dynamic physical environment missing, imbalanced, or noisy data under multimodal data scenarios. Novel applications or interfaces on multimodal data are also related topics.</p>
  
  <p>We welcome works addressing challenges from a wide range of data and sensing modalities, including but not limited to: WiFi, LIDAR, GPS, RFID, visible light communication, vibration, accelerometer, pressure, temperature, humidity, biochemistry, image, video, audio, speech, natural language, AR/VR.
    <br />
    <br />    
          </div>



           <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Important Dates </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
  <li>Submission deadline: <del>September 05, 2022, AoE</del> September 19, 2022, AoE</li>
  <li>Notification of acceptance: <del>October 03, 2022, AoE</del> October 10, 2022, AoE </li>
  <li>Deadline for camera ready version: October 17, 2022, AoE</li>
  <li>Workshop: November 06, 2022</li>
  <!--<p>Submission site coming soon!</p>-->
  <p><a href="https://cmliot2022.hotcrp.com/" class="btn btn-primary px-4 py-2">Submit Now</a></p>
  <br />
    <br />
          </div>

           <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Submission Guidelines </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
		    <!--<p>Coming Soon!</p>-->
            <p>All submissions must use the LaTeX (preferred) or Word styles found <a href="https://www.acm.org/publications/proceedings-template">here</a>. LaTeX submissions should use the acmart.cls template (sigconf option), with the default 9-pt font. We invite papers of varying length from 2 to 6 pages, plus additional pages for the reference; i.e., the reference page(s) are not counted to the limit of 6 pages. Accepted papers will be included in the ACM Digital Library and supplemental proceedings of the conference. Reviews are not double-blind, and author names and affiliations should be listed.</p>
  <br />
    <br />     
          </div>
		  
	

      <div class="col-md-12 mx-auto text-left section-heading">
             <h3 class="mb-5 text-uppercase">Invited Keynote Speakers</h3>
            </div>
        </div>
		
		
		<div class="row mb50">
					<div class="col-lg-3 col-md-3 col-sm-4 col-xs-12 wow fadeInLeft animated" data-wow-duration="500ms">
						<div class="contact-address">
							<img src="images/wen_hu.jpg" alt="Speaker" class="img-fluid">
							<h3><a style="color:#000000" href="https://github.com/wenh77/Website/wiki">Wen Hu</a></h3>
							<p>University of New South Wales</p>
						</div>
					</div>

					
					<div class="col-lg-9 col-md-9 col-sm-8 col-xs-12 wow fadeInDown animated" data-wow-duration="500ms" data-wow-delay="300ms">
						<h3>Privacy-Preserving Machine Learning in Sensor Rich IoT Systems </h3><br>
						<p>
							Abstract: Sensor-rich IoT systems are becoming ubiquitous in our lives, from smart wristbands with IMU, to smartphones with depth cameras, to low-cost embedded networked radars. These systems are providing very good alternative ways for human context detection. Yet, making the robust inference from the multi-modality raw sensor data to individual's context in the wild remains difficult. Furthermore, human context may consist of sensitive information, which needs to be protected from malicious attackers. In this talk, I will discuss my group's ongoing research on addressing these challenges with example applications in fitness, health and cybersecurity.
						</p>
						
						<p>
							Bio: Wen Hu is a professor at School of Computer Science and Engineering, the University of New South Wales (UNSW). His current research focuses on novel applications, low-power communications, security, signal processing and machine learning in Cyber Physical Systems (CPS) and Internet of Things (IoT). Hu published regularly in the top rated sensor network and mobile computing venues such as ACM/IEEE IPSN, ACM SenSys, ACM MobiCOM and ACM UbiCOMP.  He is an associate editor of ACM TOSN, the general chair of CPS-IoT Week 2020, co-chairs the program committee of ACM/IEEE IPSN 2023 and ACM Web Conference (WWW 2023, Systems and Infrastructure for Web, Mobile Web, and Web of Things track). Hu actively commercialises his research results in smart buildings and IoT, and his endeavours include Parking Spotz  and WBS Tech. Prior to joining UNSW, he was a principal research scientist and research project leader at CSIRO.
						</p>
					</div>
				</div>
				
		<div class="row mb50">
					<div class="col-lg-3 col-md-3 col-sm-4 col-xs-12 wow fadeInLeft animated" data-wow-duration="500ms">
						<div class="contact-address">
							<img src="images/duwan.jpg" alt="Speaker" class="img-fluid">
							<h3><a style="color:#000000" href="https://sites.ucmerced.edu/wdu">Wan Du</a></h3>
							<p>University of California, Merced</p>
						</div>
					</div>

					
					<div class="col-lg-9 col-md-9 col-sm-8 col-xs-12 wow fadeInDown animated" data-wow-duration="500ms" data-wow-delay="300ms">
						<h3>Arm Tracking by Multi-Modality Inertial Measurement Unit Sensors </h3><br>
						<p>
							Abstract: Arm tracking is essential for many mobile applications, such as gesture recognition, fitness training, and smart health. Smartwatches have Inertial Measurement Unit (IMU) sensors, including accelerometer, gyroscope, and magnetometer. They provide a convenient way to track the orientation and location of the wrist. Current IMU-based orientation estimation is based on a fixed multi-modality data fusion scheme that does not adapt to the data quality variation of IMU sensors. Since existing location estimation relies on the estimated orientation result, a small orientation error may cause high inaccuracy in location estimation. Moreover, these location estimation algorithms, e.g., Hidden Markov Model and Particle Filters, cannot provide real-time results due to high computation overhead. In this talk, I will introduce my group's recent research on deep learning for multi-modality data fusion of IMU sensors, which can tackle the above limitations of current solutions.
						</p>
						
						<p>
							Bio: Dr. Wan Du is an assistant professor in the Department of Computer Science and Engineering at the University of California, Merced. His research interest includes the Internet of Things, Wireless Networking Systems, Cyber Physical Systems, and Deep Reinforcement Learning. His research results have been published in top conferences (e.g., ACM SenSys, ACM/IEEE IPSN, ACM MobiCom, and IEEE INFOCOM) and journals (e.g., IEEE ToN, IEEE TMC, and ACM TOSN). He has received the best paper award in ACM SenSys 2015, the best paper runner-up awards in ACM BuildSys 2022 and IEEE DCOSS 2021, and the best demo award in IEEE SECON 2014. He was one of the Distinguished TPC Members of INFOCOM 2022, 2020, and 2018.
						</p>
					</div>
				</div>
		
		
		


        <div class="row justify-content-center">
    <h3 class="mb-5 text-uppercase">Organizers</h3>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            <h5 class="mb-5 text">Workshop Chairs (Feel free to contact us by <a href="mailto:cmliot2022@gmail.com">cmliot2022@gmail.com</a>, if you have any questions.) </h5>
            <li>Stephen Xia (University of California, Berkeley and Columbia University)</li>
            <li>Jingxiao Liu (Stanford University)</li>
            <li>Tong Yu (Adobe Research)</li>
			<li>Handong Zhao (Adobe Research)</li>
			<li>Ruiyi Zhang (Adobe Research)</li>
            <br />
            <br />
            <h5 class="mb-5 text">Steering Committee</h5>
            <li>Nicholas Lane (University of Cambridge and Samsung AI)</li>
            <li>Lina Yao (University of New South Wales)</li>
            <li>Jennifer Healey (Adobe Research)</li>
            <li>Xiaofan (Fred) Jiang (Columbia University)</li>
            <li>Hae Young Noh (Stanford University)</li>
			<li>Shijia Pan (University of California Merced)</li>
			<li>Susu Xu (Stony Brook University)</li>
            <br />
            <br />
      
      <h5 class="mb-5 text">Technical Program Committee</h5>
            <li>Winston Chen (MIT)</li>      
            <li>Karthik Dantu (University at Buffalo)</li>
            <li>Mi Zhang (Ohio State University)</li>
	    <li>Jingping Nie (Columbia University)</li>	
	    <li>Jorge Ortiz (Rutgers University)</li>	
	    <li>Yang Gao (Northwestern University)</li>	
	    <li>Shibo Zhang (HP Labs)</li>	
		<li>Wei Ma (The Hong Kong Polytechnic University)</li>	    
            <li>Yujie Wei (Meta)</li>
            <li>Bingqing Chen (Bosch Center for AI)</li>		 
            <li>Shuo Li (Flexport)</li>
			<li>Chulhong Min (Nokia Bell Labs)</li>
			<li>Zhanpeng Jin (University at Buffalo)</li>
			<li>VP Nguyen (University of Texas at Arlington)</li>
			<li>Anh Nguyen (University of Montana)</li>
			<!--<p>Coming Soon!</p>-->
      <br />
    <br />
  </div>
        <div class="col-md-12 mx-auto text-left section-heading">
		  <h3 class="mb-5 text-uppercase">Program (UTC - 4)</h3> 
          </div>
        </div>
		
		<div class="row justify-content-center">
		<div class="col-12">
		
		
	  <h5 class="mb-5 text">Zoom: <a href="https://stanford.zoom.us/j/96996423069?pwd=YklFSVQvY0tlT01PVUN1M2FnaEh2QT09">link</a></h5>  
	   

      <h5 class="mb-5 text">Welcome! (8:00 - 8:15)</h5>  
		  
      <h5 class="mb-5 text">Keynote (8:15 - 9:15), Speaker: Prof. Wan Du, University of California Merced </h5>
		  
        <h5 class="mb-6 text">Session 1: Continuous and reinforcement learning for IoT (9:30 - 10:30)</h5>
		  
		  <p><b>Towards Data-efficient Continuous Learning for Edge Video Analytics via Smart Caching</b><br/ >Lei Zhang, Guanyu Gao (Nanjing University of Science and Technology); Huaizheng Zhang (Nanyang Technological University)</p>
    
		  <p><b>Intelligent Continuous Monitoring to Handle Data Distributional Changes for IoT Systems</b><br/ >Soma Bandyopadhyay, Anish Datta, Arpan Pal (TCS Research); Srinivas Raghu Raman Gadepally (TATA Consultancy Services)</p>
  		    
		  <p><b>H-SwarmLoc: Efficient Scheduling for Localization of Heterogeneous MAV Swarm with Deep Reinforcement Learning</b><br/ >Haoyang Wang, Xuecheng Chen, Yuhan Cheng (Tsinghua University); Chenye Wu (The Chinese University of Hong Kong, Shenzhen); Fan Dang, Xinlei Chen (Tsinghua University)</p>
  
        <h5 class="mb-6 text">Session 2: Multi-modal and Multi-task learning for IoT (11:00 - 12:00)</h5>
		  
		  <p><b>GaitVibe+: Enhancing Structural Vibration-based Footstep Localization Using Temporary Cameras for In-home Gait Analysis</b><br/ >Yiwen Dong, Jingxiao Liu, Hae Young Noh (Stanford University)</p>
		  
		  
		  <p><b>Out-Clinic Pulmonary Disease Evaluation via Acoustic Sensing and Multi-task Learning on Commodity Smartphones</b><br/ >Xiangyu Yin, Kai Huang, Erick Forno, Wei Chen, Heng Huang, Wei Gao (University of Pittsburgh)</p>
  		   
		  <p><b>Discovering and Understanding Algorithmic Biases in Autonomous Pedestrian Trajectory Predictions</b><br/ >Andrew Bae, Susu Xu (Stony Brook University)</p>
   
  		  
		  <p><p>
		  
      <h5 class="mb-5 text">Keynote (2:00 - 3:00), Speaker: Prof. Wen Hu, The University of New South Wales </h5>
		  
        <h5 class="mb-6 text">Session 3: Learning with limited labeled data for IoT (3:30 - 4:30)</h5>
		  
		  <p><b>Near-real-time Seismic Human Fatality Information Retrieval from Social Media with Few-shot Large-Language Models</b><br/ >James Hou, Susu Xu (Stony Brook University)</p>
		  
		  <p><b>Memory-Efficient Domain Incremental Learning for Internet of Things</b><br/ >Yuqing Zhao, Divya Saxena, Jiannong Cao (The Hong Kong Polytechnic University)</p>
    
		  <p><b>Riemannian Geometric Instance Filtering for Transfer Learning in Brain-Computer Interfaces</b><br/ >Qianxin Hui, Xiaolin Liu (Beihang University); Yang Li (Tsinghua University); Susu Xu (Stony Brook University); Shuailei Zhang, Ying Sun, Shuai Wang (Beihang University); Xinlei Chen (Tsinghua University); Dezhi Zheng (Beihang University)</p>
		  
	 <h5 class="mb-5 text">Business Meeting, Closing, and Awards (4:30)</h5>
  
  <br />
    <br />
          </div>
		  </div>
        <!--<div class="row justify-content-center">
          <div class="col-12">
            
      <h5 class="mb-5 text">Welcome! (9:00 - 9:15)</h5>  
      <h5 class="mb-5 text">Keynote (9:15 - 10:00), Speaker: Archan Misra, Singapore Management University </h5>
      
      
        <h5 class="mb-6 text">Session 1: Multimodal Sensing and Learning for Smart Health (10:00 - 11:00)</h5>
        <li>RCH: Robust Calibration Based on Historical Data for Low-Cost Air Quality Sensor Deployments, Guodong Li, Rui Ma, Xinyu Liu, Yue Wang, Lin Zhang</li>
        <li>Spinal curve assessment of idiopathic scoliosis with a small dataset via a multi-scale keypoint estimation approach, Liu Tianyu, Yang Yukang, Wang Yu, Sun Ming, Fan Wenhui, Wu Cheng, Cody Bunger </li>
        <li>Digital Twin - A Machine Learning Approach to Predict Individual Stress Levels in Extreme Environments, Dr. Constantin Stefan Scheuermann, Thomas Binderberger, Nadine von Frankenberg, Dr. med Andreas Werner</li> 
      
      <br>
      <h5 class="mb-5 text">Lunch (11:00 - 12:30) </h5>
  
      <h5 class="mb-6 text">Session 2: Continual and Multimodal Learning for Smart Building (12:30 - 13:30)</h5> 
      <li>Space Utilization and Activity Recognition using 3D Stereo Vision Camera inside an Educational Building, Anooshmita Das Das, Krister, Mikkel Baun Kjærgaard</li> 
      <li>Fine-grained Activities Recognition with Coarse-grained Labeled Multi-modal Data, Zhizhang Hu, Tong Yu, Yue Zhang, Shijia Pan</li> 
      <li>Accurate Trajectory Prediction in a Smart Building using Recurrent Neural Networks, Anooshmita Das, Mikkel Baun Kjærgaard</li> 
      
      <br>

       <h5 class="mb-6 text">Session 3: System Evaluation and Calibration (13:30 - 13:50) </h6> 
      <li>Evaluation of Federated Learning Aggregation Algorithms, Sannara Ek, François Portet, Philippe Lalanda and German Vega</li>   
      <br>

      <h5 class="mb-5 text">Registration/Doors Close (14:00) </h5>  
            
      <br>
      Note: For each paper presentation, there are 15 minutes for presentation and 5 minutes for Q&A.
    
          </div>
        </div>   -->    
      </div>
    </div>

    
    

          
          
        <div class="row pt-5 mt-5 text-center">
          <div class="col-md-12">
            <p>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            Copyright &copy; <script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script>document.write(new Date().getFullYear());</script> All Rights Reserved | This template is made with <i class="icon-heart text-primary" aria-hidden="true"></i> by <a href="https://colorlib.com" target="_blank" >Colorlib</a>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            </p>
          </div>
          
        </div>
      </div>
    </footer>
  </div>

  <script src="js/jquery-3.3.1.min.js"></script>
  <script src="js/jquery-migrate-3.0.1.min.js"></script>
  <script src="js/jquery-ui.js"></script>
  <script src="js/popper.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/owl.carousel.min.js"></script>
  <script src="js/jquery.stellar.min.js"></script>
  <script src="js/jquery.countdown.min.js"></script>
  <script src="js/jquery.magnific-popup.min.js"></script>
  <script src="js/bootstrap-datepicker.min.js"></script>
  <script src="js/aos.js"></script>

  
  <script src="js/mediaelement-and-player.min.js"></script>

  <script src="js/main.js"></script>
    

  <script>
      document.addEventListener('DOMContentLoaded', function() {
                var mediaElements = document.querySelectorAll('video, audio'), total = mediaElements.length;

                for (var i = 0; i < total; i++) {
                    new MediaElementPlayer(mediaElements[i], {
                        pluginPath: 'https://cdn.jsdelivr.net/npm/mediaelement@4.2.7/build/',
                        shimScriptAccess: 'always',
                        success: function () {
                            var target = document.body.querySelectorAll('.player'), targetTotal = target.length;
                            for (var j = 0; j < targetTotal; j++) {
                                target[j].style.visibility = 'visible';
                            }
                  }
                });
                }
            });
    </script>

  </body>
</html>
