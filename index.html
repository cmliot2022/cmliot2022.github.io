<!DOCTYPE html>
<html lang="en">
  <head>
    <title>CML-IOT 2022</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link href="https://fonts.googleapis.com/css?family=Amatic+SC:400,700|Work+Sans:300,400,700" rel="stylesheet">
    <link rel="stylesheet" href="fonts/icomoon/style.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/magnific-popup.css">
    <link rel="stylesheet" href="css/jquery-ui.css">
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/bootstrap-datepicker.css">
    <link rel="stylesheet" href="css/animate.css">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mediaelement@4.2.7/build/mediaelementplayer.min.css">
    
    <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
  
    <link rel="stylesheet" href="css/aos.css">

    <link rel="stylesheet" href="css/style.css">
    
  </head>
  <body>
  
  <div class="site-wrap">

    <div class="site-mobile-menu">
      <div class="site-mobile-menu-header">
        <div class="site-mobile-menu-close mt-3">
          <span class="icon-close2 js-menu-toggle"></span>
        </div>
      </div>
      <div class="site-mobile-menu-body"></div>
    </div> <!-- .site-mobile-menu -->
    
    
    <div class="site-navbar-wrap js-site-navbar bg-white">
      
      <div class="container">
        <div class="site-navbar bg-light">
          <div class="py-1">
            <div class="row align-items-center">
              <div class="col-8">
                <h2 class="mb-0 site-logo"><a href="index.html">CML-IOT 2022</a></h2>
              </div>
              
            </div>
          </div>
        </div>
      </div>
    </div>
  
    
    
      
    <div class="site-blocks-cover overlay" style="background-image: url(images/Charles_River.jpg);" data-aos="fade" data-stellar-background-ratio="0.5">
      <div class="container">
        <div class="row align-items-center justify-content-center">
          <div class="col-md-10 text-center" data-aos="fade">
            <font size="10" color="white">The Fourth Workshop on Continual and Multimodal Learning for Internet of Things</font>
            <p class="mb-5">November 06, 2022 &bullet; Boston, Massachusetts, USA</p>
      <p class="mb-6">Co-Located with <a href="https://sensys.acm.org/2022/" style="color:white">SenSys 2022</a></p>
          </div>
        </div>
      </div>
    </div>  

<a name="more"></a>

      

    <div class="site-section">
      <div class="container">
		  
        
        <div class="row">
          <div class="col-md-12 mx-auto text-left section-heading">
		     <h3 class="mb-5 text-uppercase">Previous Editions: <a href="https://cmliot2021.github.io">CML-IOT'21</a>, <a href="https://cmliot2020.github.io">CML-IOT'20</a>, <a href="https://cmliot2019.github.io/index.html">CML-IOT'19</a></h3> 
             <h3 class="mb-5 text-uppercase">About CML-IOT </h3> 
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
		  
		  
  <p>The growth of the Internet of Things (IoT) has brought an ever-growing number of connected sensors, continuously streaming large quantities of multimodal data. These data come from a wide range of different sensing modalities and have distinct statistical characteristics over time, which are hardly captured by traditional learning methods. Continual and multimodal learning allows the integration, adaptation, and generalization of knowledge learned from experiential and heterogeneous data to new situations. Therefore, continual and multimodal learning is an important step to enable efficient information inference for IoT systems. CML-IOT welcomes works from diverse communities that introduce algorithmic and systemic approaches to leverage continual learning on multimodal data for applications and real-world computing systems for the Internet of Things.</p>
  <br />
    <br />
          </div>




          <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Call for Papers </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            <p>The Workshop on Continual and Multimodal Learning for Internet of Things (CML-IOT 2022) aims to explore the intersection of continual machine learning and multimodal modeling. The Internet of Things (IoT) has brought an ever-growing amount of multimodal sensing data (e.g., natural language, speech, image, video, audio, virtual reality, WiFi, GPS, RFID, vibration). The statistical properties of this data vary significantly over time and depending on the sensing modality; these differences are hardly captured by conventional learning methods. Continual and multimodal learning allows the integration, adaptation and generalization of knowledge learned from experiential and heterogenous data to new situations. Therefore, continual and multimodal learning is an important step to improve the estimation, utilization, and security of real-world data from IoT systems.</p>
			
			<p>We welcome works addressing these issues in different applications and domains, such as natural language processing, computer vision, human-centric sensing, smart cities, health, etc. We aim to bring together researchers from different areas to establish a multidisciplinary community and share the latest research. We focus on novel learning methods that can be applied on streaming multimodal data with applications to the Internet of Things. Topics of interest include, but are not limited to:</p>
  <li>Continual learning </li>
  <li>Transfer learning </li>
  <li>Federated learning </li>
  <li>Few-shot learning </li>
  <li>Multi-task learning </li>
  <li>Reinforcement learning </li>
  <li>Learning without forgetting </li>
  <li>Individual and/or institutional privacy </li>
  <li>Methods and architectures for partitioning on-device and off-device learning </li>
  <li>Managing high volumes of data flow</li>

        
   <br />
   
  <p>We also welcome continual learning methods that target: data distribution changes caused by the fast-changing dynamic physical environment missing, imbalanced, or noisy data under multimodal data scenarios. Novel applications or interfaces on multimodal data are also related topics.</p>
  
  <p>We welcome works addressing challenges from a wide range of data and sensing modalities, including but not limited to: WiFi, LIDAR, GPS, RFID, visible light communication, vibration, accelerometer, pressure, temperature, humidity, biochemistry, image, video, audio, speech, natural language, AR/VR.
    <br />
    <br />    
          </div>



           <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Important Dates </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
  <li>Submission deadline: September 05, 2022, AoE
  <li>Notification of acceptance: October 03, 2022, AoE </li>
  <li>Deadline for camera ready version: October 17, 2022, AoE</li>
  <li>Workshop: November 06, 2022</li>
  <!--<p>Submission site coming soon!</p>-->
  <p><a href="https://cmliot2022.hotcrp.com/" class="btn btn-primary px-4 py-2">Submit Now</a></p>
  <br />
    <br />
          </div>

           <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Submission Guidelines </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
		    <!--<p>Coming Soon!</p>-->
            <p>All submissions must use the LaTeX (preferred) or Word styles found <a href="https://www.acm.org/publications/proceedings-template">here</a>.. LaTeX submissions should use the acmart.cls template (sigconf option), with the default 9-pt font. We invite papers of varying length from 2 to 6 pages, plus additional pages for the reference; i.e., the reference page(s) are not counted to the limit of 6 pages. Accepted papers will be included in the ACM Digital Library and supplemental proceedings of the conference. Reviews are not double-blind, and author names and affiliations should be listed.</p>
  <br />
    <br />     
          </div>

      <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Keynote </h3>
            </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">

<!--<h5 class="mb-5 text">Edge-Assisted Collaborative IoT Machine Intelligence, Speaker: Archan Misra, Singapore Management University</h5>-->

<div class="row justify-content-center">
          <div class="col-12">

<!--<p>Abstract: To support real-time & sustainable machine intelligence that exploits the rapid growth in sensor deployments and wearable devices (e.g., inertial, radar), there is a need to optimize the execution of machine learning (ML) pipelines on resource-constrained embedded devices. Through this talk, I shall introduce the paradigm of collaborative machine intelligence (CMI), where the sensing and ML pipelines on individual wearable and IoT devices collaborate, in real-time. I shall describe why CMI also requires the evolution of an edge node, from merely a resource for offloaded computation, to a platform for coordination among IoT devices. Using an exemplar video surveillance application, I will describe how IoT-based CMI can provide dramatic reductions in sensing energy, while also improving accuracy and minimizing communication overheads. I shall also describe how CMI can be leveraged to support (a) ultra-low power (and even battery-less operation) of wearable devices and (b) resource-efficient instruction comprehension for natural human-machine interfaces.  </p>
  
<p>Speaker Bio: Archan Misra is Vice Provost (Research) and Professor of Computer Science at Singapore Management University (SMU). He is the Director of SMU’s Center for Applied Smart-Nation Analytics (CASA), which is developing pervasive technologies for smart city infrastructure and applications. Archan has led a number of multi-million dollar, large-scale research initiatives at SMU, including the LiveLabs research center, and is a current recipient of the prestigious Investigator grant (from Singapore’s National Research Foundation) for sustainable man-machine interaction intelligence. Over a 20+ year research career spanning both academics and industry (at IBM Research and Bellcore), Archan has published on, and practically deployed, technologies spanning wireless networking, mobile & wearable sensing and urban mobility analytics. His current research interests lie in ultra-low energy execution of machine intelligence algorithms using wearable and IoT devices. Archan holds a Ph.D. from the University of Maryland at College Park, and chaired the IEEE Computer Society's Technical Committee on Computer Communications (TCCC) from 2005-2007. </p>-->

<p>Coming Soon!</p>
<br>
<br>
  </div>
        </div>


        </div>
        <div class="row justify-content-center">
    
     <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Organizers</h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            <h5 class="mb-5 text">Workshop Chairs (Feel free to contact us by <a href="mailto:cmliot2022@gmail.com">cmliot2022@gmail.com</a>, if you have any questions.) </h5>
            <li>Stephen Xia (Columbia University)</li>
            <li>Jingxiao Liu (Stanford University)</li>
            <li>Tong Yu (Adobe Research)</li>
			<li>Handong Zhao (Adobe Research)</li>
			<li>Ruiyi Zhang (Adobe Research)</li>
            <br />
            <br />
            <h5 class="mb-5 text">Steering Committee</h5>
            <li>Nicholas Lane (University of Cambridge and Samsung AI)</li>
            <li>Lina Yao (University of New South Wales)</li>
            <li>Jennifer Healey (Adobe Research)</li>
            <li>Xiaofan (Fred) Jiang (Columbia University)</li>
            <li>Hae Young Noh (Stanford University)</li>
			<li>Shijia Pan (University of California Merced)</li>
			<li>Susu Xu (Stony Brook University)</li>
            <br />
            <br />
      
      <h5 class="mb-5 text">Technical Program Committee</h5>
            <li>Winston Chen (MIT)</li>      
            <li>Karthik Dantu (University at Buffalo)</li>
            <li>Mi Zhang (Michigan State University)</li>
	    <li>JingPing Nie (Columbia University)</li>	
	    <li>Jorge Ortiz (Rutgers University)</li>	
	    <li>Yang Gao (Northwestern University)</li>	
	    <li>Shibo Zhang (HP Labs)</li>	
	<li>Wei Ma (The Hong Kong Polytechnic University)</li>	    
            <li>Yujie Wei (Meta)</li>
            <li>Bingqing Chen (Bosch Center for AI)</li>		 
            <li>Shuo Li (Flexport)</li>
			<!--<p>Coming Soon!</p>-->
      <br />
    <br />
  </div>
        <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Program</h3>
			<p>TBD</p>
          </div>
        </div>
        <!--<div class="row justify-content-center">
          <div class="col-12">
            
      <h5 class="mb-5 text">Welcome! (9:00 - 9:15)</h5>  
      <h5 class="mb-5 text">Keynote (9:15 - 10:00), Speaker: Archan Misra, Singapore Management University </h5>
      
      
        <h5 class="mb-6 text">Session 1: Multimodal Sensing and Learning for Smart Health (10:00 - 11:00)</h5>
        <li>RCH: Robust Calibration Based on Historical Data for Low-Cost Air Quality Sensor Deployments, Guodong Li, Rui Ma, Xinyu Liu, Yue Wang, Lin Zhang</li>
        <li>Spinal curve assessment of idiopathic scoliosis with a small dataset via a multi-scale keypoint estimation approach, Liu Tianyu, Yang Yukang, Wang Yu, Sun Ming, Fan Wenhui, Wu Cheng, Cody Bunger </li>
        <li>Digital Twin - A Machine Learning Approach to Predict Individual Stress Levels in Extreme Environments, Dr. Constantin Stefan Scheuermann, Thomas Binderberger, Nadine von Frankenberg, Dr. med Andreas Werner</li> 
      
      <br>
      <h5 class="mb-5 text">Lunch (11:00 - 12:30) </h5>
  
      <h5 class="mb-6 text">Session 2: Continual and Multimodal Learning for Smart Building (12:30 - 13:30)</h5> 
      <li>Space Utilization and Activity Recognition using 3D Stereo Vision Camera inside an Educational Building, Anooshmita Das Das, Krister, Mikkel Baun Kjærgaard</li> 
      <li>Fine-grained Activities Recognition with Coarse-grained Labeled Multi-modal Data, Zhizhang Hu, Tong Yu, Yue Zhang, Shijia Pan</li> 
      <li>Accurate Trajectory Prediction in a Smart Building using Recurrent Neural Networks, Anooshmita Das, Mikkel Baun Kjærgaard</li> 
      
      <br>

       <h5 class="mb-6 text">Session 3: System Evaluation and Calibration (13:30 - 13:50) </h6> 
      <li>Evaluation of Federated Learning Aggregation Algorithms, Sannara Ek, François Portet, Philippe Lalanda and German Vega</li>   
      <br>

      <h5 class="mb-5 text">Registration/Doors Close (14:00) </h5>  
            
      <br>
      Note: For each paper presentation, there are 15 minutes for presentation and 5 minutes for Q&A.
    
          </div>
        </div>   -->    
      </div>
    </div>

    
    

          
          
        <div class="row pt-5 mt-5 text-center">
          <div class="col-md-12">
            <p>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            Copyright &copy; <script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script>document.write(new Date().getFullYear());</script> All Rights Reserved | This template is made with <i class="icon-heart text-primary" aria-hidden="true"></i> by <a href="https://colorlib.com" target="_blank" >Colorlib</a>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            </p>
          </div>
          
        </div>
      </div>
    </footer>
  </div>

  <script src="js/jquery-3.3.1.min.js"></script>
  <script src="js/jquery-migrate-3.0.1.min.js"></script>
  <script src="js/jquery-ui.js"></script>
  <script src="js/popper.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/owl.carousel.min.js"></script>
  <script src="js/jquery.stellar.min.js"></script>
  <script src="js/jquery.countdown.min.js"></script>
  <script src="js/jquery.magnific-popup.min.js"></script>
  <script src="js/bootstrap-datepicker.min.js"></script>
  <script src="js/aos.js"></script>

  
  <script src="js/mediaelement-and-player.min.js"></script>

  <script src="js/main.js"></script>
    

  <script>
      document.addEventListener('DOMContentLoaded', function() {
                var mediaElements = document.querySelectorAll('video, audio'), total = mediaElements.length;

                for (var i = 0; i < total; i++) {
                    new MediaElementPlayer(mediaElements[i], {
                        pluginPath: 'https://cdn.jsdelivr.net/npm/mediaelement@4.2.7/build/',
                        shimScriptAccess: 'always',
                        success: function () {
                            var target = document.body.querySelectorAll('.player'), targetTotal = target.length;
                            for (var j = 0; j < targetTotal; j++) {
                                target[j].style.visibility = 'visible';
                            }
                  }
                });
                }
            });
    </script>

  </body>
</html>
